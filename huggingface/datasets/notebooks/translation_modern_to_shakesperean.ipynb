{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r ./../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_DATASETS_NAME = \"harpreetsahota/modern-to-shakesperean-translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pathlib\n",
    "\n",
    "CORRELATION_ID = uuid.uuid4().hex[:4].upper()\n",
    "OUTPUT_DIR = f\"./../data/{CORRELATION_ID}-{HF_DATASETS_NAME.split('/')[-1]}\"\n",
    "\n",
    "pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"HF datasets name: {HF_DATASETS_NAME}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Correlation ID: {CORRELATION_ID}\")\n",
    "\n",
    "# HF datasets name: harpreetsahota/modern-to-shakesperean-translation\n",
    "# Output directory: ./../data/4314-modern-to-shakesperean-translation\n",
    "# Correlation ID: ABCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets: train, validation, test\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(HF_DATASETS_NAME)  # doctest: +IGNORE_RESULT\n",
    "\n",
    "print(f\"datasets: {[k for k in datasets]}\")\n",
    "\n",
    "# datasets: ['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_testvalid = datasets['train'].train_test_split(test_size=0.2)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'validation': test_valid['train'],\n",
    "    'test': test_valid['test']\n",
    "})\n",
    "\n",
    "print(split_datasets)\n",
    "\n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['modern', 'shakespearean'],\n",
    "#         num_rows: 219\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['modern', 'shakespearean'],\n",
    "#         num_rows: 27\n",
    "#     })\n",
    "#     test: Dataset({\n",
    "#         features: ['modern', 'shakespearean'],\n",
    "#         num_rows: 28\n",
    "#     })\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in each split of the dataset\n",
    "import copy\n",
    "import json\n",
    "\n",
    "messages_template = {\"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Translate this.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"{prompt}\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"{completion}\"}]\n",
    "}\n",
    "\n",
    "chat_message_dataset = {}\n",
    "for split in split_datasets:\n",
    "    print(f\"Processing split: {split}\")\n",
    "    records = []\n",
    "    for i in range(len(split_datasets[split])):\n",
    "        # print(f\"Processing record: {i}\")\n",
    "        compiled_messages = copy.deepcopy(messages_template)\n",
    "        compiled_messages['messages'][1]['content'] = split_datasets[split][i][\"modern\"]\n",
    "        compiled_messages['messages'][2]['content'] = split_datasets[split][i][\"shakespearean\"]\n",
    "        records.append(compiled_messages)\n",
    "    chat_message_dataset[split] = records\n",
    "\n",
    "print(\"=\" * 16, \"\\n\", \"chat_message_dataset:\")\n",
    "print(json.dumps(chat_message_dataset,indent=2)[:500])\n",
    "\n",
    "# Processing split: train\n",
    "# Processing split: validation\n",
    "# Processing split: test\n",
    "# ================ \n",
    "#  chat_message_dataset:\n",
    "# {\n",
    "#   \"train\": [\n",
    "#     {\n",
    "#       \"messages\": [\n",
    "#         {\n",
    "#           \"role\": \"system\",\n",
    "#           \"content\": \"Translate this.\"\n",
    "#         },\n",
    "#         {\n",
    "#           \"role\": \"user\",\n",
    "#           \"content\": \"That's dope\"\n",
    "#         },\n",
    "#         {\n",
    "#           \"role\": \"assistant\",\n",
    "#           \"content\": \"Verily, 'tis wondrous\"\n",
    "#         }\n",
    "#       ]\n",
    "#     },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file paths\n",
    "output_files = {\n",
    "    'train': f\"{OUTPUT_DIR}/{CORRELATION_ID}-train.jsonl\",\n",
    "    'validation': f\"{OUTPUT_DIR}/{CORRELATION_ID}-validation.jsonl\",\n",
    "    'test': f\"{OUTPUT_DIR}/{CORRELATION_ID}-test.jsonl\"\n",
    "}\n",
    "\n",
    "# Write each split to its respective file in JSONL format\n",
    "for split, records in chat_message_dataset.items():\n",
    "    with open(output_files[split], 'w') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Data exported to {OUTPUT_DIR} in JSONL format.\")\n",
    "\n",
    "# Data exported to ./../data/ABCD-modern-to-shakesperean-translation in JSONL format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost estimation\n",
    "ref. https://cookbook.openai.com/examples/chat_finetuning_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initial dataset stats\n",
    "dataset = chat_message_dataset['train']\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Num examples: 219\n",
    "# First example:\n",
    "# {'role': 'system', 'content': 'Translate this.'}\n",
    "# {'role': 'user', 'content': \"That's dope\"}\n",
    "# {'role': 'assistant', 'content': \"Verily, 'tis wondrous\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "    \n",
    "# No errors found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Num examples missing system message: 0\n",
    "# Num examples missing user message: 0\n",
    "\n",
    "# #### Distribution of num_messages_per_example:\n",
    "# min / max: 3, 3\n",
    "# mean / median: 3.0, 3.0\n",
    "# p5 / p95: 3.0, 3.0\n",
    "\n",
    "# #### Distribution of num_total_tokens_per_example:\n",
    "# min / max: 23, 119\n",
    "# mean / median: 42.337899543378995, 42.0\n",
    "# p5 / p95: 29.0, 53.0\n",
    "\n",
    "# #### Distribution of num_assistant_tokens_per_example:\n",
    "# min / max: 3, 51\n",
    "# mean / median: 13.812785388127853, 13.0\n",
    "# p5 / p95: 6.800000000000001, 20.200000000000017\n",
    "\n",
    "# 0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Dataset has ~9272 tokens that will be charged for during training\n",
    "# By default, you'll train for 3 epochs on this dataset\n",
    "# By default, you'll be charged for ~27816 tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
