{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Finetuning (PEFT) with Low-Level Adaptation (LORA) using HuggingFace PEFT on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv==1.0.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn==1.5.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: datasets==2.20.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: transformers==4.43.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.43.2)\n",
      "Requirement already satisfied: evaluate==0.4.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.4.2)\n",
      "Requirement already satisfied: peft==0.12.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: numba==0.60.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.60.0)\n",
      "Requirement already satisfied: huggingface_hub==0.24.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3)) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: packaging in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from transformers==4.43.2->-r requirements.txt (line 4)) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from transformers==4.43.2->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from transformers==4.43.2->-r requirements.txt (line 4)) (0.4.3)\n",
      "Requirement already satisfied: psutil in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from peft==0.12.0->-r requirements.txt (line 6)) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from peft==0.12.0->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from peft==0.12.0->-r requirements.txt (line 6)) (0.33.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from numba==0.60.0->-r requirements.txt (line 8)) (0.43.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from huggingface_hub==0.24.2->-r requirements.txt (line 9)) (4.12.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (2024.6.2)\n",
      "Requirement already satisfied: sympy in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (2.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941151683
    }
   },
   "outputs": [],
   "source": [
    "# input constants\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "HF_PRETRAINED_MODEL_NAME = \"google/flan-t5-base\" # \"distilbert/distilbert-base-uncased\"\n",
    "HF_DATASET_NAME = \"knkarthick/dialogsum\"\n",
    "\n",
    "TRAINING_EPOCHS = int(os.getenv('TRAINING_EPOCHS'))\n",
    "TRAINING_BATCH_SIZE = int(os.getenv('TRAINING_BATCH_SIZE'))\n",
    "TRAINING_LEARNING_RATE = float(os.getenv('TRAINING_LEARNING_RATE'))\n",
    "TRAINING_DEVICE = 'gpu' # one of ['cpu', 'gpu', 'mps']\n",
    "\n",
    "LORA_TARGET_MODULES=[\n",
    "    \"q\", \n",
    "    \"v\"\n",
    "]\n",
    "LORA_R = int(os.getenv('LORA_R'))\n",
    "LORA_ALPHA = int(os.getenv('LORA_ALPHA'))\n",
    "LORA_DROPOUT = float(os.getenv('LORA_DROPOUT'))\n",
    "\n",
    "OUTPUT_DIRECTORY = os.path.join('trained', HF_PRETRAINED_MODEL_NAME)\n",
    "HUGGINGFACE_REPO_ID = os.getenv('HUGGINGFACE_REPO_ID')\n",
    "\n",
    "if TRAINING_DEVICE == 'gpu':\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941154263
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF pretrained model name: google/flan-t5-base\n",
      "HF datasets name: knkarthick/dialogsum\n",
      "LORA r: 8\n",
      "LORA alpha: 32\n",
      "LORA droupout: 0.1\n",
      "epochs: 5\n",
      "batch_size: 64\n",
      "learning rate (lr): 0.001\n",
      "Using gpu device\n"
     ]
    }
   ],
   "source": [
    "print(f\"HF pretrained model name: {HF_PRETRAINED_MODEL_NAME}\")\n",
    "print(f\"HF dataset name: {HF_DATASET_NAME}\")\n",
    "\n",
    "print(f\"epochs: {TRAINING_EPOCHS}\")\n",
    "print(f\"batch_size: {TRAINING_BATCH_SIZE}\")\n",
    "print(f\"learning rate (lr): {TRAINING_LEARNING_RATE}\")\n",
    "\n",
    "print(f\"LORA r: {LORA_R}\")\n",
    "print(f\"LORA alpha: {LORA_ALPHA}\")\n",
    "print(f\"LORA droupout: {LORA_DROPOUT}\")\n",
    "\n",
    "print(f\"Using {TRAINING_DEVICE} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941186999
    }
   },
   "outputs": [],
   "source": [
    "# download datasets: train, validation, test\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(HF_DATASET_NAME)  # doctest: +IGNORE_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941187076
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: ['train', 'validation', 'test']\n",
      "len(train): 12460\n",
      "len(validation): 500\n",
      "len(test): 1500\n",
      "train dataset: Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "    num_rows: 12460\n",
      "})\n",
      "train dataset features: {'id': Value(dtype='string', id=None), 'dialogue': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None), 'topic': Value(dtype='string', id=None)}\n",
      "topics (8521 unique), first 10: ['phone numbers', 'having breakfast', 'married life', 'receipt', ' Biography of Dean', 'cultural shock', 'fashion awards', 'concentration problems', 'independent life', 'A fishy website']\n",
      "Example (0): {\n",
      "  \"id\": \"train_0\",\n",
      "  \"dialogue\": \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
      "  \"summary\": \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
      "  \"topic\": \"get a check-up\"\n",
      "}\n",
      "Example (1): {\n",
      "  \"id\": \"train_1\",\n",
      "  \"dialogue\": \"#Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\",\n",
      "  \"summary\": \"Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.\",\n",
      "  \"topic\": \"vaccines\"\n",
      "}\n",
      "Example (2): {\n",
      "  \"id\": \"train_2\",\n",
      "  \"dialogue\": \"#Person1#: Excuse me, did you see a set of keys?\\n#Person2#: What kind of keys?\\n#Person1#: Five keys and a small foot ornament.\\n#Person2#: What a shame! I didn't see them.\\n#Person1#: Well, can you help me look for it? That's my first time here.\\n#Person2#: Sure. It's my pleasure. I'd like to help you look for the missing keys.\\n#Person1#: It's very kind of you.\\n#Person2#: It's not a big deal.Hey, I found them.\\n#Person1#: Oh, thank God! I don't know how to thank you, guys.\\n#Person2#: You're welcome.\",\n",
      "  \"summary\": \"#Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\",\n",
      "  \"topic\": \"find keys\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(f\"dataset: {[k for k in dataset]}\")\n",
    "topics = set()\n",
    "for dataset_key in dataset:\n",
    "    print(f\"len({dataset_key}): {len(dataset[dataset_key])}\")\n",
    "    [topics.add(t) for t in dataset[dataset_key]['topic']]\n",
    "print(f\"train dataset: {dataset['train']}\")\n",
    "print(f\"train dataset features: {dataset['train'].features}\")\n",
    "print(f\"topics ({len(topics)} unique), first 10: {[topics.pop() for i in range(10)]}\")\n",
    "for i in range(3):\n",
    "    print(f\"Example ({i}): {json.dumps(dataset['train'][i], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1721941320624
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(HF_PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1721941324149
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download model\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=HF_PRETRAINED_MODEL_NAME\n",
    ")\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941329974
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==INPUT TEXT==:\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      "==OUTPUT==:\n",
      "<pad> Dr. Hawkins is here to help.</s>\n",
      "==EXPECTED==:\n",
      "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "# test inference\n",
    "input_text = dataset['train'][0]['dialogue']\n",
    "print(f\"==INPUT TEXT==:\\n{input_text}\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = base_model.generate(inputs=input_ids, max_length=4000)\n",
    "print(f\"==OUTPUT==:\\n{tokenizer.decode(outputs[0])}\")\n",
    "print(f\"==EXPECTED==:\\n{dataset['train'][0]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941374429
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "# Hugging Face Transformers models expect tokenized input, \n",
    "# rather than the text in the downloaded data.\n",
    "def tokenize_dataset(dataset):\n",
    "    prompt = [f\"Summarize the following dialogue:\\n\\n{dialogue}\\n\\nSummary:\" \n",
    "              for dialogue in dataset[\"dialogue\"]]\n",
    "    dataset['input_ids'] = tokenizer(\n",
    "        prompt,\n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt').input_ids\n",
    "    dataset['labels'] = tokenizer(\n",
    "        dataset['summary'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt').input_ids\n",
    "    return dataset\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    tokenize_dataset, \n",
    "    batched=True,\n",
    "    remove_columns=['id', 'topic', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941377208
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: ['train', 'validation', 'test']\n",
      "len(train): 12460\n",
      "len(validation): 500\n",
      "len(test): 1500\n",
      "train dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 12460\n",
      "})\n",
      "train dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Example (0): {\n",
      "  \"input_ids\": [\n",
      "    12198,\n",
      "    1635,\n",
      "    1737,\n",
      "    8,\n",
      "    826,\n",
      "    7478,\n",
      "    10,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    2018,\n",
      "    6,\n",
      "    1363,\n",
      "    5,\n",
      "    3931,\n",
      "    5,\n",
      "    27,\n",
      "    31,\n",
      "    51,\n",
      "    7582,\n",
      "    12833,\n",
      "    77,\n",
      "    7,\n",
      "    5,\n",
      "    1615,\n",
      "    33,\n",
      "    25,\n",
      "    270,\n",
      "    469,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    27,\n",
      "    435,\n",
      "    34,\n",
      "    133,\n",
      "    36,\n",
      "    3,\n",
      "    9,\n",
      "    207,\n",
      "    800,\n",
      "    12,\n",
      "    129,\n",
      "    3,\n",
      "    9,\n",
      "    691,\n",
      "    18,\n",
      "    413,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    2163,\n",
      "    6,\n",
      "    168,\n",
      "    6,\n",
      "    25,\n",
      "    43,\n",
      "    29,\n",
      "    31,\n",
      "    17,\n",
      "    141,\n",
      "    80,\n",
      "    21,\n",
      "    305,\n",
      "    203,\n",
      "    5,\n",
      "    148,\n",
      "    225,\n",
      "    43,\n",
      "    80,\n",
      "    334,\n",
      "    215,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    27,\n",
      "    214,\n",
      "    5,\n",
      "    27,\n",
      "    2320,\n",
      "    38,\n",
      "    307,\n",
      "    38,\n",
      "    132,\n",
      "    19,\n",
      "    1327,\n",
      "    1786,\n",
      "    6,\n",
      "    572,\n",
      "    281,\n",
      "    217,\n",
      "    8,\n",
      "    2472,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    1548,\n",
      "    6,\n",
      "    8,\n",
      "    200,\n",
      "    194,\n",
      "    12,\n",
      "    1792,\n",
      "    2261,\n",
      "    21154,\n",
      "    19,\n",
      "    12,\n",
      "    253,\n",
      "    91,\n",
      "    81,\n",
      "    135,\n",
      "    778,\n",
      "    5,\n",
      "    264,\n",
      "    653,\n",
      "    12,\n",
      "    369,\n",
      "    44,\n",
      "    709,\n",
      "    728,\n",
      "    3,\n",
      "    9,\n",
      "    215,\n",
      "    21,\n",
      "    39,\n",
      "    293,\n",
      "    207,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    8872,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    1563,\n",
      "    140,\n",
      "    217,\n",
      "    270,\n",
      "    5,\n",
      "    696,\n",
      "    2053,\n",
      "    11,\n",
      "    11581,\n",
      "    320,\n",
      "    1399,\n",
      "    5,\n",
      "    2321,\n",
      "    3,\n",
      "    9,\n",
      "    1659,\n",
      "    6522,\n",
      "    6,\n",
      "    754,\n",
      "    5,\n",
      "    531,\n",
      "    25,\n",
      "    7269,\n",
      "    6,\n",
      "    1363,\n",
      "    5,\n",
      "    3931,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    2163,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    14627,\n",
      "    53,\n",
      "    19,\n",
      "    8,\n",
      "    1374,\n",
      "    1137,\n",
      "    13,\n",
      "    5084,\n",
      "    1874,\n",
      "    11,\n",
      "    842,\n",
      "    1994,\n",
      "    6,\n",
      "    25,\n",
      "    214,\n",
      "    5,\n",
      "    148,\n",
      "    310,\n",
      "    225,\n",
      "    10399,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    27,\n",
      "    31,\n",
      "    162,\n",
      "    1971,\n",
      "    3986,\n",
      "    13,\n",
      "    648,\n",
      "    6,\n",
      "    68,\n",
      "    27,\n",
      "    131,\n",
      "    54,\n",
      "    31,\n",
      "    17,\n",
      "    1727,\n",
      "    12,\n",
      "    4583,\n",
      "    8,\n",
      "    7386,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    1548,\n",
      "    6,\n",
      "    62,\n",
      "    43,\n",
      "    2287,\n",
      "    11,\n",
      "    128,\n",
      "    11208,\n",
      "    24,\n",
      "    429,\n",
      "    199,\n",
      "    5,\n",
      "    27,\n",
      "    31,\n",
      "    195,\n",
      "    428,\n",
      "    25,\n",
      "    72,\n",
      "    251,\n",
      "    274,\n",
      "    25,\n",
      "    1175,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    8872,\n",
      "    6,\n",
      "    2049,\n",
      "    2472,\n",
      "    5,\n",
      "    20698,\n",
      "    10,\n",
      "    1,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0\n",
      "  ],\n",
      "  \"labels\": [\n",
      "    1363,\n",
      "    5,\n",
      "    3931,\n",
      "    31,\n",
      "    7,\n",
      "    652,\n",
      "    3,\n",
      "    9,\n",
      "    691,\n",
      "    18,\n",
      "    413,\n",
      "    6,\n",
      "    11,\n",
      "    7582,\n",
      "    12833,\n",
      "    77,\n",
      "    7,\n",
      "    7786,\n",
      "    7,\n",
      "    376,\n",
      "    12,\n",
      "    43,\n",
      "    80,\n",
      "    334,\n",
      "    215,\n",
      "    5,\n",
      "    12833,\n",
      "    77,\n",
      "    7,\n",
      "    31,\n",
      "    195,\n",
      "    428,\n",
      "    128,\n",
      "    251,\n",
      "    81,\n",
      "    70,\n",
      "    2287,\n",
      "    11,\n",
      "    11208,\n",
      "    12,\n",
      "    199,\n",
      "    1363,\n",
      "    5,\n",
      "    3931,\n",
      "    10399,\n",
      "    10257,\n",
      "    5,\n",
      "    1,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0\n",
      "  ]\n",
      "}\n",
      "Example (1): {\n",
      "  \"input_ids\": [\n",
      "    12198,\n",
      "    1635,\n",
      "    1737,\n",
      "    8,\n",
      "    826,\n",
      "    7478,\n",
      "    10,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    8774,\n",
      "    8667,\n",
      "    5,\n",
      "    13156,\n",
      "    6,\n",
      "    149,\n",
      "    43,\n",
      "    25,\n",
      "    118,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    8774,\n",
      "    707,\n",
      "    5,\n",
      "    2737,\n",
      "    7,\n",
      "    5,\n",
      "    1142,\n",
      "    1399,\n",
      "    2763,\n",
      "    25,\n",
      "    5,\n",
      "    11066,\n",
      "    63,\n",
      "    11,\n",
      "    27,\n",
      "    33,\n",
      "    270,\n",
      "    21,\n",
      "    112,\n",
      "    12956,\n",
      "    7,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    4242,\n",
      "    168,\n",
      "    5,\n",
      "    1563,\n",
      "    31,\n",
      "    7,\n",
      "    217,\n",
      "    6,\n",
      "    1315,\n",
      "    12,\n",
      "    112,\n",
      "    24639,\n",
      "    1368,\n",
      "    6,\n",
      "    11066,\n",
      "    63,\n",
      "    65,\n",
      "    1204,\n",
      "    112,\n",
      "    6907,\n",
      "    32,\n",
      "    6,\n",
      "    2255,\n",
      "    17,\n",
      "    152,\n",
      "    302,\n",
      "    11,\n",
      "    216,\n",
      "    7768,\n",
      "    17,\n",
      "    159,\n",
      "    272,\n",
      "    6562,\n",
      "    5,\n",
      "    216,\n",
      "    19,\n",
      "    968,\n",
      "    767,\n",
      "    625,\n",
      "    6,\n",
      "    78,\n",
      "    3,\n",
      "    88,\n",
      "    19,\n",
      "    788,\n",
      "    21,\n",
      "    216,\n",
      "    7768,\n",
      "    17,\n",
      "    159,\n",
      "    71,\n",
      "    6,\n",
      "    16451,\n",
      "    102,\n",
      "    32,\n",
      "    226,\n",
      "    11,\n",
      "    1212,\n",
      "    9,\n",
      "    7,\n",
      "    965,\n",
      "    6562,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    363,\n",
      "    81,\n",
      "    15612,\n",
      "    5303,\n",
      "    11,\n",
      "    283,\n",
      "    22028,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    1548,\n",
      "    6,\n",
      "    27,\n",
      "    54,\n",
      "    163,\n",
      "    428,\n",
      "    376,\n",
      "    175,\n",
      "    21,\n",
      "    230,\n",
      "    6,\n",
      "    11,\n",
      "    227,\n",
      "    3,\n",
      "    9,\n",
      "    1158,\n",
      "    13,\n",
      "    1274,\n",
      "    27,\n",
      "    54,\n",
      "    24235,\n",
      "    8,\n",
      "    880,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    6902,\n",
      "    6,\n",
      "    248,\n",
      "    5,\n",
      "    7582,\n",
      "    6,\n",
      "    27,\n",
      "    317,\n",
      "    27,\n",
      "    92,\n",
      "    164,\n",
      "    174,\n",
      "    3,\n",
      "    9,\n",
      "    2255,\n",
      "    17,\n",
      "    152,\n",
      "    302,\n",
      "    22205,\n",
      "    5,\n",
      "    2506,\n",
      "    97,\n",
      "    27,\n",
      "    530,\n",
      "    34,\n",
      "    47,\n",
      "    2087,\n",
      "    17310,\n",
      "    203,\n",
      "    977,\n",
      "    55,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    101,\n",
      "    56,\n",
      "    691,\n",
      "    69,\n",
      "    3187,\n",
      "    11,\n",
      "    27,\n",
      "    31,\n",
      "    195,\n",
      "    43,\n",
      "    8,\n",
      "    10444,\n",
      "    24235,\n",
      "    11,\n",
      "    8,\n",
      "    22205,\n",
      "    38,\n",
      "    168,\n",
      "    5,\n",
      "    852,\n",
      "    6,\n",
      "    754,\n",
      "    1520,\n",
      "    11066,\n",
      "    63,\n",
      "    31,\n",
      "    7,\n",
      "    2939,\n",
      "    4095,\n",
      "    6,\n",
      "    48,\n",
      "    164,\n",
      "    3,\n",
      "    11026,\n",
      "    3,\n",
      "    9,\n",
      "    385,\n",
      "    5,\n",
      "    20698,\n",
      "    10,\n",
      "    1,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0\n",
      "  ],\n",
      "  \"labels\": [\n",
      "    8667,\n",
      "    13156,\n",
      "    1217,\n",
      "    11066,\n",
      "    63,\n",
      "    21,\n",
      "    112,\n",
      "    12956,\n",
      "    7,\n",
      "    5,\n",
      "    707,\n",
      "    5,\n",
      "    2737,\n",
      "    7,\n",
      "    11642,\n",
      "    8,\n",
      "    1368,\n",
      "    11,\n",
      "    258,\n",
      "    1527,\n",
      "    11066,\n",
      "    63,\n",
      "    3,\n",
      "    9,\n",
      "    12956,\n",
      "    5,\n",
      "    1,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0\n",
      "  ]\n",
      "}\n",
      "Example (2): {\n",
      "  \"input_ids\": [\n",
      "    12198,\n",
      "    1635,\n",
      "    1737,\n",
      "    8,\n",
      "    826,\n",
      "    7478,\n",
      "    10,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    20335,\n",
      "    1074,\n",
      "    140,\n",
      "    6,\n",
      "    410,\n",
      "    25,\n",
      "    217,\n",
      "    3,\n",
      "    9,\n",
      "    356,\n",
      "    13,\n",
      "    9060,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    363,\n",
      "    773,\n",
      "    13,\n",
      "    9060,\n",
      "    58,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    9528,\n",
      "    9060,\n",
      "    11,\n",
      "    3,\n",
      "    9,\n",
      "    422,\n",
      "    2418,\n",
      "    12917,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    363,\n",
      "    3,\n",
      "    9,\n",
      "    12447,\n",
      "    55,\n",
      "    27,\n",
      "    737,\n",
      "    31,\n",
      "    17,\n",
      "    217,\n",
      "    135,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    1548,\n",
      "    6,\n",
      "    54,\n",
      "    25,\n",
      "    199,\n",
      "    140,\n",
      "    320,\n",
      "    21,\n",
      "    34,\n",
      "    58,\n",
      "    466,\n",
      "    31,\n",
      "    7,\n",
      "    82,\n",
      "    166,\n",
      "    97,\n",
      "    270,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    10625,\n",
      "    5,\n",
      "    94,\n",
      "    31,\n",
      "    7,\n",
      "    82,\n",
      "    5565,\n",
      "    5,\n",
      "    27,\n",
      "    31,\n",
      "    26,\n",
      "    114,\n",
      "    12,\n",
      "    199,\n",
      "    25,\n",
      "    320,\n",
      "    21,\n",
      "    8,\n",
      "    3586,\n",
      "    9060,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    94,\n",
      "    31,\n",
      "    7,\n",
      "    182,\n",
      "    773,\n",
      "    13,\n",
      "    25,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    94,\n",
      "    31,\n",
      "    7,\n",
      "    59,\n",
      "    3,\n",
      "    9,\n",
      "    600,\n",
      "    1154,\n",
      "    5,\n",
      "    3845,\n",
      "    63,\n",
      "    6,\n",
      "    27,\n",
      "    435,\n",
      "    135,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    10,\n",
      "    3359,\n",
      "    6,\n",
      "    2763,\n",
      "    601,\n",
      "    55,\n",
      "    27,\n",
      "    278,\n",
      "    31,\n",
      "    17,\n",
      "    214,\n",
      "    149,\n",
      "    12,\n",
      "    2763,\n",
      "    25,\n",
      "    6,\n",
      "    3413,\n",
      "    5,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    10,\n",
      "    148,\n",
      "    31,\n",
      "    60,\n",
      "    2222,\n",
      "    5,\n",
      "    20698,\n",
      "    10,\n",
      "    1,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0\n",
      "  ],\n",
      "  \"labels\": [\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    536,\n",
      "    4663,\n",
      "    31,\n",
      "    7,\n",
      "    479,\n",
      "    21,\n",
      "    3,\n",
      "    9,\n",
      "    356,\n",
      "    13,\n",
      "    9060,\n",
      "    11,\n",
      "    987,\n",
      "    7,\n",
      "    21,\n",
      "    1713,\n",
      "    345,\n",
      "    13515,\n",
      "    357,\n",
      "    4663,\n",
      "    31,\n",
      "    7,\n",
      "    199,\n",
      "    12,\n",
      "    253,\n",
      "    135,\n",
      "    5,\n",
      "    1,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0,\n",
      "    0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(f\"encoded dataset: {[k for k in encoded_dataset]}\")\n",
    "for dataset_key in encoded_dataset:\n",
    "    print(f\"len({dataset_key}): {len(encoded_dataset[dataset_key])}\")\n",
    "print(f\"train dataset: {encoded_dataset['train']}\")\n",
    "print(f\"train dataset features: {encoded_dataset['train'].features}\")\n",
    "for i in range(3):\n",
    "    print(f\"Example ({i}): {json.dumps(encoded_dataset['train'][i], indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941379641
    }
   },
   "outputs": [],
   "source": [
    "# configure LoRA\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # defines the expected fields of the tokenized dataset\n",
    "    target_modules=LORA_TARGET_MODULES,  # model modules to apply LoRA to\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941381663
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    }
   ],
   "source": [
    "# wrap model with PEFT config\n",
    "from peft import get_peft_model\n",
    "\n",
    "peft_wrapped_model = get_peft_model(base_model, peft_config)\n",
    "peft_wrapped_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Transformers for Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1721941386203
    }
   },
   "outputs": [],
   "source": [
    "# data loader/collator to batch input in training and evaluation datasets\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941388782
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# configure evaluation metrics \n",
    "# in addition to the default `loss` metric that the `Trainer` computes\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "evaluation_module = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred, evaluation_module=evaluation_module):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return evaluation_module.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941393185
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# [OPTIONAL] clean up the GPU memory\n",
    "if TRAINING_DEVICE == 'gpu':\n",
    "    from numba import cuda\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941393339
    }
   },
   "outputs": [],
   "source": [
    "# train job config\n",
    "# Hugging Face training configuration tools can be used to configure a Trainer.\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIRECTORY,\n",
    "    \n",
    "    #do_train=True,\n",
    "    #do_eval=True,\n",
    "\n",
    "    num_train_epochs=TRAINING_EPOCHS,\n",
    "    per_device_train_batch_size=TRAINING_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TRAINING_BATCH_SIZE,\n",
    "    learning_rate=TRAINING_LEARNING_RATE,\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    #gradient_accumulation_steps=2,  # default 1\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"f1\"\n",
    "    \n",
    "    #fp16=True,  # lower precision\n",
    "    # use_ipex=True if DEVICE == 'cpu' else False,  # use Intel extension for PyTorch\n",
    "    use_cpu=True if TRAINING_DEVICE == 'cpu' else False  # False will use CUDA or MPS if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941396223
    }
   },
   "outputs": [],
   "source": [
    "# The Trainer classes require the user to provide: 1) Metrics 2) A base model 3) A training configuration\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_wrapped_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721941403821
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721939469374
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "peft_wrapped_model.save_pretrained(OUTPUT_DIRECTORY)\n",
    "tokenizer.save_pretrained(OUTPUT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721939469382
    }
   },
   "outputs": [],
   "source": [
    "# save on Huggingface\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "peft_wrapped_model.push_to_hub(HUGGINGFACE_REPO_ID)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
